{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b6iF4cae8it"
   },
   "source": [
    "### Install needed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NKxYHQB7vYWK",
    "outputId": "f8804c59-35b3-4ef9-c59a-950436d952e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.3.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.4.2 (from gradio)\n",
      "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m667.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.6.1)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading gradio-5.3.0-py3-none-any.whl (56.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
      "Downloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, rouge, python-multipart, orjson, markupsafe, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "Successfully installed aiofiles-23.2.1 fastapi-0.115.3 ffmpy-0.4.0 gradio-5.3.0 gradio-client-1.4.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.1 markupsafe-2.1.5 orjson-3.10.10 pydub-0.25.1 python-multipart-0.0.12 rouge-1.0.1 ruff-0.7.1 semantic-version-2.10.0 starlette-0.41.0 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge transformers gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa6coLTLfBPR"
   },
   "source": [
    "### Import needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_OMVKhli2b3r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, BartTokenizer, MarianMTModel, MarianTokenizer\n",
    "from rouge import Rouge\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iRdVf4LJ2jKa",
    "outputId": "d034178e-acdb-48ff-ca40-413516a0bde5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device >= 0:\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIIspk6ag0B2"
   },
   "source": [
    "## AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PfX7mGD7gyHX"
   },
   "outputs": [],
   "source": [
    "# Load BART model and tokenizer for text summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Load translation model and tokenizer for English to Finnish\n",
    "translation_model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-fi')\n",
    "translation_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fi')\n",
    "\n",
    "# Create Rouge metric object for scoring\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu0igH8bfHPq"
   },
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rYZdjDAd8-_z"
   },
   "outputs": [],
   "source": [
    "# Convert character-based length to token-based length\n",
    "def char_to_token_length(text, char_length):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    token_length = int(len(tokens) * (char_length / len(text)))\n",
    "    return token_length\n",
    "\n",
    "# Print the number of tokens in the input text\n",
    "def print_token_count(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(f\"Input text token count: {len(tokens)}\")\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mDVr7WAo2cF"
   },
   "source": [
    "## Finnish translater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yqj-v6TMg6qW"
   },
   "outputs": [],
   "source": [
    "# Function to translate English text to Finnish\n",
    "def translate_to_finnish(text):\n",
    "    translated_tokens = translation_model.generate(**translation_tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "    translated_text = translation_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfIsMYYNozvw"
   },
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CdSlVJ30vV2L"
   },
   "outputs": [],
   "source": [
    "# Summarization function\n",
    "def summarize_text(text, min_length=500, max_length=2000, target_language=\"English\"):\n",
    "    if not text.strip():\n",
    "        return \"Please provide a non-empty text.\", 0, \"N/A\", \"N/A\", 0\n",
    "\n",
    "    input_length_chars = len(text)\n",
    "    token_count = print_token_count(text)\n",
    "\n",
    "    # Check if input text exceeds token limit\n",
    "    max_tokens = 1000\n",
    "    if token_count > max_tokens:\n",
    "        return f\"Input text ({token_count} tokens) exceeds the maximum token limit of {max_tokens}. Please reduce the text and try again.\", 0, \"N/A\", \"N/A\", input_length_chars\n",
    "\n",
    "    # Convert character length to token length\n",
    "    min_length_tokens = char_to_token_length(text, min_length)\n",
    "    max_length_tokens = char_to_token_length(text, max_length)\n",
    "\n",
    "    # Ensure max_length isn't shorter than min_length\n",
    "    if max_length_tokens < min_length_tokens:\n",
    "        return \"Max length must be greater than or equal to the min length.\", 0, \"N/A\", \"N/A\", input_length_chars\n",
    "\n",
    "    try:\n",
    "        # Summarize text using the BART model\n",
    "        # combined_summary = summarizer(text, min_length=min_length_tokens, max_length=max_length_tokens, do_sample=False)[0]['summary_text']\n",
    "        combined_summary = summarizer(\n",
    "            text,\n",
    "            min_length=min_length_tokens, # Minimum number of tokens for the generated summary\n",
    "            max_length=max_length_tokens, # Maximum number of tokens for the generated summary\n",
    "            length_penalty=1.0, # Length penalty to control summary length; 1.0 means no penalty\n",
    "            do_sample=False, # Disable random sampling, making generation deterministic\n",
    "            num_beams=12, # Number of beams in beam search for better accuracy; higher values increase quality\n",
    "            early_stopping=True # Stop generation when all beams finish, ensuring a concise output\n",
    "            )[0]['summary_text']\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\", 0, \"N/A\", \"N/A\", input_length_chars\n",
    "\n",
    "    if not combined_summary:\n",
    "        return \"Error in summarizing the text.\", 0, \"N/A\", \"N/A\", input_length_chars\n",
    "\n",
    "    # Translate summary to Finnish if needed\n",
    "    if target_language == \"Finnish\":\n",
    "        combined_summary = translate_to_finnish(combined_summary)\n",
    "\n",
    "    # Calculate Rouge score\n",
    "    rouge_scores = rouge.get_scores(combined_summary, text)\n",
    "    rouge_1 = rouge_scores[0]['rouge-1']['f'] * 100  # F1 score\n",
    "    rouge_L = rouge_scores[0]['rouge-l']['f'] * 100  # F1 score\n",
    "\n",
    "    return combined_summary, rouge_1, input_length_chars, rouge_L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y3yidWEJw1xL"
   },
   "outputs": [],
   "source": [
    "# Character count function\n",
    "def get_character_count(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLkY4xLPpAgP"
   },
   "source": [
    "## Adjusting summary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jBYKJGu9zpBP"
   },
   "outputs": [],
   "source": [
    "# Functions for preset summary size based on character count\n",
    "def set_small_summary(text):\n",
    "    input_length_chars = len(text)\n",
    "    min_length = max(100, int(input_length_chars * 0.1))\n",
    "    max_length = max(300, int(input_length_chars * 0.2))\n",
    "    return min_length, max_length\n",
    "\n",
    "def set_medium_summary(text):\n",
    "    input_length_chars = len(text)\n",
    "    min_length = max(100, int(input_length_chars * 0.2))\n",
    "    max_length = max(300, int(input_length_chars * 0.3))\n",
    "    return min_length, max_length\n",
    "\n",
    "def set_large_summary(text):\n",
    "    input_length_chars = len(text)\n",
    "    min_length = max(100, int(input_length_chars * 0.3))\n",
    "    max_length = max(300, int(input_length_chars * 0.4))\n",
    "    return min_length, max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiwsItsqpESK"
   },
   "source": [
    "# Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "Iext15fVwRuW",
    "outputId": "c0fb34dc-7c82-45a5-bff9-1f31870c96d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://f4f575079356479e26.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f4f575079356479e26.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text token count: 894\n",
      "Input text token count: 518\n",
      "Input text token count: 518\n",
      "Input text token count: 518\n",
      "Input text token count: 518\n",
      "Input text token count: 518\n",
      "Input text token count: 518\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://f4f575079356479e26.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Text Summarization with BART\n",
    "    ### Enter the text to summarize and adjust the length settings.\"\"\")\n",
    "\n",
    "    # Textbox for input\n",
    "    textbox = gr.Textbox(lines=5, label=\"Input Text\", placeholder=\"Enter your text here...\")\n",
    "    char_count_output = gr.Number(label=\"Input Length (Characters)\", value=0, interactive=False)\n",
    "    min_length_slider = gr.Slider(minimum=100, maximum=3000, value=100, label=\"Min Length (Characters)\", visible=False)\n",
    "    max_length_slider = gr.Slider(minimum=300, maximum=5000, value=300, label=\"Max Length (Characters)\", visible=False)\n",
    "\n",
    "    # Language selection radio buttons\n",
    "    language_selection = gr.Radio(\n",
    "        choices=[\"English\", \"Finnish\"], label=\"Select output language\", value=\"English\"\n",
    "    )\n",
    "\n",
    "    summary_size = gr.Radio(\n",
    "        choices=[\"Small\", \"Medium\", \"Large\"],\n",
    "        label=\"Select Summary Size\",\n",
    "    )\n",
    "\n",
    "    summary_output = gr.Textbox(label=\"Summary\", visible=True)\n",
    "    rouge_1_output = gr.Number(label=\"ROUGE-1 Score\", visible=False)\n",
    "    rouge_L_output = gr.Number(label=\"ROUGE-L Score\", visible=False)\n",
    "\n",
    "    submit_btn = gr.Button(\"Summarize\")\n",
    "\n",
    "    # Define function to adjust sliders based on selected summary size\n",
    "    def adjust_summary_size(text, selected_size):\n",
    "        if selected_size == \"Small\":\n",
    "            return set_small_summary(text)\n",
    "        elif selected_size == \"Medium\":\n",
    "            return set_medium_summary(text)\n",
    "        elif selected_size == \"Large\":\n",
    "            return set_large_summary(text)\n",
    "\n",
    "    # Bind functions to Gradio components\n",
    "    submit_btn.click(\n",
    "        summarize_text,\n",
    "        inputs=[textbox, min_length_slider, max_length_slider, language_selection],\n",
    "        outputs=[summary_output, rouge_1_output, char_count_output, rouge_L_output]\n",
    "    )\n",
    "\n",
    "    # Update character count output\n",
    "    textbox.change(fn=lambda text: len(text), inputs=textbox, outputs=char_count_output)\n",
    "\n",
    "    # Update sliders when radio button changes\n",
    "    summary_size.change(\n",
    "        fn=adjust_summary_size,\n",
    "        inputs=[textbox, summary_size],\n",
    "        outputs=[min_length_slider, max_length_slider]\n",
    "    )\n",
    "\n",
    "iface.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
